{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minhvt34/cbm_codes_open/blob/master/Transformer_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWhRU1Kt3Zx-",
        "outputId": "d3de9e21-1a9e-40ff-ce08-1842ed13560c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformer-time-series-prediction'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 35 (delta 13), reused 23 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (35/35), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/oliverguhr/transformer-time-series-prediction.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "inCpaeHXLgMc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "from matplotlib import pyplot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bEWQm-lTM1oj"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvmHOdwO3eLz"
      },
      "outputs": [],
      "source": [
        "# S is the  source sequence length\n",
        "# T is the target sequence length\n",
        "# N is the batch size\n",
        "# E is the feature number\n",
        "\n",
        "#src = torch.rand((10, 32, 512)) (S, N, E)\n",
        "#tgt = torch.rand((20, 32, 512)) (T, N, E)\n",
        "#out = transformer_model(src, tgt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf')\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Jy4Y-brvBF8F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import pytorch_lightning as pl\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "  !pip install --quiet pytorch-lightning>=1.4\n",
        "  import pytorch_lightning as pl\n",
        "\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
      ],
      "metadata": {
        "id": "-B5fz4K0CvMX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b9HeHbRTx9v0"
      },
      "outputs": [],
      "source": [
        "input_window = 100 # number of input steps\n",
        "output_window = 5 # number of prediction steps, in this model its fixed to one\n",
        "batch_size = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "  d_k = q.size()[-1]\n",
        "  attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "  attn_logits = attn_logits/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "  attention = F.softmax(attn_logits, dim=-1)\n",
        "  values = torch.matmul(attention, v)\n",
        "  return values, attention"
      ],
      "metadata": {
        "id": "oEISogcD_ykm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len, d_k = 3, 2\n",
        "pl.seed_everything(42)\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ],
      "metadata": {
        "id": "HHpTd6ssAy_F",
        "outputId": "3a013674-22d6-4de2-9ede-a84aa7894085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " tensor([[ 0.3367,  0.1288],\n",
            "        [ 0.2345,  0.2303],\n",
            "        [-1.1229, -0.1863]])\n",
            "K\n",
            " tensor([[ 2.2082, -0.6380],\n",
            "        [ 0.4617,  0.2674],\n",
            "        [ 0.5349,  0.8094]])\n",
            "V\n",
            " tensor([[ 1.1103, -1.6898],\n",
            "        [-0.9890,  0.9580],\n",
            "        [ 1.3221,  0.8172]])\n",
            "Values\n",
            " tensor([[ 0.5698, -0.1520],\n",
            "        [ 0.5379, -0.0265],\n",
            "        [ 0.2246,  0.5556]])\n",
            "Attention\n",
            " tensor([[0.4028, 0.2886, 0.3086],\n",
            "        [0.3538, 0.3069, 0.3393],\n",
            "        [0.1303, 0.4630, 0.4067]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, embed_dim, num_heads):\n",
        "      super().__init__()\n",
        "      assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads\"\n",
        "\n",
        "      self.embed_dim = embed_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.head_dim = embed_dim // num_heads\n",
        "\n",
        "      self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "      self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "      self._reset_parameters()\n",
        "\n",
        "  def _reset_parameters(self):\n",
        "      # Original Transformer initialization\n",
        "      nn.init.xavier_normal_(self.qkv_proj.weight)\n",
        "      self.qkv_proj.bias.data.fill_(0)\n",
        "      nn.init.xavier_normal_(self.o_proj.weight)\n",
        "      self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "  def forward(self, x, mask=None, return_attention=False):\n",
        "    batch_size, seq_lenght, embed_dim = x.size()\n",
        "    qkv = self.qkv_proj(x)\n",
        "\n",
        "    # Separate Q, K, V from linear output\n",
        "    qkv = qkv.reshape(batch_size, seq_lenght, self.num_heads, 3*self.head_dim)\n",
        "    qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, Seqlen, Dims]\n",
        "    q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "    #Determine value outputs\n",
        "    values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "    values = values.permute(0, 2, 1, 3) #[Batch, SeqLen, Head, Dims]\n",
        "    values = values.reshape(batch_size, seq_lenght, embed_dim)\n",
        "    o = self.o_proj(values)\n",
        "\n",
        "    if return_attention:\n",
        "      return o, attention\n",
        "    else:\n",
        "      return o"
      ],
      "metadata": {
        "id": "7CCuQUoLDwKl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DX8hY8EJyeZx"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()       \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        #pe.requires_grad = False\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x[0,0,0])\n",
        "        # print(self.pe[0,0,0])\n",
        "        # print(x.size(1), x.size(0))\n",
        "        # print(self.pe[:x.size(0), :].shape)\n",
        "        #x + self.pe[:x.size(0), :]\n",
        "        #x = x + self.pe[:, :x.size(0)]\n",
        "        #ts = x + self.pe[:x.size(0), :]\n",
        "        #print(\"after: \", ts[0,0,0])\n",
        "        return x + self.pe[:x.size(0), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K1yv8aNL-OmQ"
      },
      "outputs": [],
      "source": [
        "class TransAm(nn.Module):\n",
        "    def __init__(self,feature_size=250,num_layers=1,dropout=0.1):\n",
        "        super(TransAm, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        \n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(feature_size)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
        "        self.decoder = nn.Linear(feature_size,1)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1    \n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self,src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qfAip4tTGafq"
      },
      "outputs": [],
      "source": [
        "# if window is 100 and prediction step is 1\n",
        "# in -> [0..99]\n",
        "# target -> [1..100]\n",
        "\n",
        "def create_inout_sequences(input_data, tw):\n",
        "    inout_seq = []\n",
        "    L = len(input_data)\n",
        "    for i in range(L-tw):\n",
        "        train_seq = np.append(input_data[i:i+tw][:-output_window] , output_window * [0])\n",
        "        train_label = input_data[i:i+tw]\n",
        "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
        "        inout_seq.append((train_seq ,train_label))\n",
        "    return torch.FloatTensor(inout_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uryQOA7iYSwZ"
      },
      "outputs": [],
      "source": [
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1RIBYLl2aJTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40e5014-7ed5-4848-f751-e72223264cdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(480,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
        "df = pd.read_csv('/content/drive/MyDrive/2003.10.22.12.06.24', sep='\\t', names=col_names)\n",
        "series = df['b1_ch1']\n",
        "\n",
        "# looks like normalizing input values curtial for the model\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
        "amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
        "#amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "samples = 20000\n",
        "train_data = amplitude[:samples]\n",
        "test_data = amplitude[samples:]\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CZIm56DmH-lC"
      },
      "outputs": [],
      "source": [
        "def get_data():\n",
        "  # time = np.arange(0, 10000, 0.1)\n",
        "  # amplitude = np.sin(time) + np.sin(time*0.05) + np.sin(time*0.12)*np.random.normal(-0.2, 0.2, len(time))\n",
        "\n",
        "  #loading weather data from a file\n",
        "  #series = read_csv('/content/2003.10.22.12.06.24', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
        "  col_names = ['b1_ch1', 'b1_ch2', 'b2_ch3', 'b2_ch4', 'b3_ch5', 'b3_ch6', 'b4_ch7', 'b4_ch8']\n",
        "  df = pd.read_csv('/content/drive/MyDrive/2003.10.22.12.06.24', sep='\\t', names=col_names)\n",
        "  series = df['b1_ch1']\n",
        "  \n",
        "  # looks like normalizing input values curtial for the model\n",
        "  scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
        "  amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
        "  #amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "  samples = 20000\n",
        "  train_data = amplitude[:samples]\n",
        "  test_data = amplitude[samples:]\n",
        "\n",
        "  # convert our train data into a pytorch train tensor\n",
        "  #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
        "  # todo: add comment.. \n",
        "  train_sequence = create_inout_sequences(train_data,input_window)\n",
        "  train_sequence = train_sequence[:-output_window] #todo: fix hack? -> din't think this through, looks like the last n sequences are to short, so I just remove them. Hackety Hack.. \n",
        "\n",
        "  #test_data = torch.FloatTensor(test_data).view(-1) \n",
        "  test_data = create_inout_sequences(test_data,input_window)\n",
        "  test_data = test_data[:-output_window] #todo: fix hack?\n",
        "\n",
        "  return train_sequence.to(device),test_data.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fQQFo58kLVJo"
      },
      "outputs": [],
      "source": [
        "def get_batch(source, i,batch_size):\n",
        "    seq_len = min(batch_size, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]    \n",
        "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
        "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
        "    return input, target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import optimizer"
      ],
      "metadata": {
        "id": "Qro9CNP61JzH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hndWNXrpA2z_"
      },
      "outputs": [],
      "source": [
        "def train(train_data):\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
        "        data, targets = get_batch(train_data, i,batch_size)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)        \n",
        "\n",
        "        if calculate_loss_over_all_values:\n",
        "            loss = criterion(output, targets)\n",
        "        else:\n",
        "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
        "    \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = int(len(train_data) / batch_size / 5)\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.6f} | {:5.2f} ms | '\n",
        "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // batch_size, lr_scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "42UQuPdPwcXj"
      },
      "outputs": [],
      "source": [
        "calculate_loss_over_all_values = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jImB1quuRFZL"
      },
      "outputs": [],
      "source": [
        "def plot_and_loss(eval_model, data_source,epoch):\n",
        "    eval_model.eval() \n",
        "    total_loss = 0.\n",
        "    test_result = torch.Tensor(0)    \n",
        "    truth = torch.Tensor(0)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data_source) - 1):\n",
        "            data, target = get_batch(data_source, i,1)\n",
        "            # look like the model returns static values for the output window\n",
        "            output = eval_model(data)    \n",
        "            if calculate_loss_over_all_values:                                \n",
        "                total_loss += criterion(output, target).item()\n",
        "            else:\n",
        "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
        "            \n",
        "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
        "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
        "            \n",
        "    #test_result = test_result.cpu().numpy()\n",
        "    len(test_result)\n",
        "\n",
        "    pyplot.plot(test_result,color=\"red\")\n",
        "    pyplot.plot(truth[:20000],color=\"blue\")\n",
        "    pyplot.plot(test_result-truth,color=\"green\")\n",
        "    pyplot.grid(True, which='both')\n",
        "    pyplot.axhline(y=0, color='k')\n",
        "    pyplot.savefig('/content/transformer-epoch%d.png'%epoch)\n",
        "    pyplot.close()\n",
        "    \n",
        "    return total_loss / i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Zp6qtpBVRgv7"
      },
      "outputs": [],
      "source": [
        "# predict the next n steps based on the input data \n",
        "def predict_future(eval_model, data_source,steps):\n",
        "    eval_model.eval() \n",
        "    total_loss = 0.\n",
        "    test_result = torch.Tensor(0)    \n",
        "    truth = torch.Tensor(0)\n",
        "    _ , data = get_batch(data_source, 0,1)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, steps,1):\n",
        "            input = torch.clone(data[-input_window:])\n",
        "            input[-output_window:] = 0     \n",
        "            output = eval_model(data[-input_window:])                        \n",
        "            data = torch.cat((data, output[-1:]))\n",
        "            \n",
        "    data = data.cpu().view(-1)\n",
        "    \n",
        "\n",
        "    pyplot.plot(data,color=\"red\")       \n",
        "    pyplot.plot(data[:input_window],color=\"blue\")\n",
        "    pyplot.grid(True, which='both')\n",
        "    pyplot.axhline(y=0, color='k')\n",
        "    pyplot.savefig('/content/results/transformer-future%d.png'%steps)\n",
        "    pyplot.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j4Wcpss6Rh4g"
      },
      "outputs": [],
      "source": [
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    eval_batch_size = 1000\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
        "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
        "            output = eval_model(data)            \n",
        "            if calculate_loss_over_all_values:\n",
        "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
        "            else:                                \n",
        "                total_loss += len(data[0])* criterion(output[-output_window:], targets[-output_window:]).cpu().item()            \n",
        "    return total_loss / len(data_source)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uGcYhCU0BpVW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "OGPym5R50vKd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
        "    \n",
        "    def __init__(self, optimizer, warmup, max_iters):\n",
        "        self.warmup = warmup\n",
        "        self.max_num_iters = max_iters\n",
        "        super().__init__(optimizer)\n",
        "        \n",
        "    def get_lr(self):\n",
        "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
        "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
        "    \n",
        "    def get_lr_factor(self, epoch):\n",
        "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
        "        if epoch <= self.warmup:\n",
        "            lr_factor *= epoch * 1.0 / self.warmup\n",
        "        return lr_factor"
      ],
      "metadata": {
        "id": "VD0dH3wyLt0F"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9U_MSZ1RmCB"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = get_data()\n",
        "model = TransAm().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "#lr = 0.005 \n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=50, max_iters=2000)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 100 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_data)\n",
        "    \n",
        "    if(epoch % 10 is 0):\n",
        "        val_loss = plot_and_loss(model, val_data,epoch)\n",
        "        predict_future(model, val_data,200)\n",
        "    else:\n",
        "        val_loss = evaluate(model, val_data)\n",
        "   \n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    #if val_loss < best_val_loss:\n",
        "    #    best_val_loss = val_loss\n",
        "    #    best_model = model\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    #scheduler.step() \n",
        "\n",
        "#src = torch.rand(input_window, batch_size, 1) # (source sequence length,batch size,feature number) \n",
        "#out = model(src)\n",
        "#\n",
        "#print(out)\n",
        "#print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDvKy0gHKnVP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class WeibullLossRMSE(nn.Module):\n",
        "    \"\"\"\n",
        "    y_hat       : predicted RUL\n",
        "    y           : true RUL\n",
        "    y_days      : true age (in days)\n",
        "    lambda_mod  : lambda modifier\n",
        "    eta         : characteristic life\n",
        "    beta        : shape parameter for weibull\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eps=1e-8):\n",
        "        super(WeibullLossRMSE, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "\n",
        "    def forward(self, y_hat, y, y_days, lambda_mod=2.0, eta=90.0, beta=2.0):\n",
        "\n",
        "        y_hat_days = (y_days + y) - y_hat\n",
        "\n",
        "        # remove any \"inf\" values from when divided by zero\n",
        "        y_hat_days = y_hat_days[torch.isfinite(y_hat_days)]\n",
        "\n",
        "        def weibull_cdf(t, eta, beta):\n",
        "            \"weibull CDF function\"\n",
        "            return 1.0 - torch.exp(-1.0 * ((t / eta) ** beta))\n",
        "\n",
        "        cdf = weibull_cdf(y_days, eta, beta)\n",
        "        cdf_hat = weibull_cdf(y_hat_days, eta, beta)\n",
        "\n",
        "        return lambda_mod * torch.sqrt(torch.mean(cdf_hat - cdf) ** 2 + self.eps)\n",
        "\n",
        "\n",
        "class WeibullLossRMSLE(nn.Module):\n",
        "    \"\"\"\n",
        "    y_hat       : predicted RUL\n",
        "    y           : true RUL\n",
        "    y_days      : true age (in days)\n",
        "    lambda_mod  : lambda modifier\n",
        "    eta         : characteristic life\n",
        "    beta        : shape parameter for weibull\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eps=1e-8):\n",
        "        super(WeibullLossRMSLE, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "\n",
        "    def forward(self, y_hat, y, y_days, lambda_mod=2.0, eta=90.0, beta=2.0):\n",
        "\n",
        "        y_hat_days = (y_days + y) - y_hat\n",
        "\n",
        "        # remove any \"inf\" values from when divided by zero\n",
        "        y_hat_days = y_hat_days[torch.isfinite(y_hat_days)]\n",
        "\n",
        "        def weibull_cdf(t, eta, beta):\n",
        "            \"weibull CDF function\"\n",
        "            return 1.0 - torch.exp(-1.0 * ((t / eta) ** beta))\n",
        "\n",
        "        cdf = weibull_cdf(y_days, eta, beta)\n",
        "        cdf_hat = weibull_cdf(y_hat_days, eta, beta)\n",
        "\n",
        "        return lambda_mod * torch.sqrt(torch.mean(torch.log(cdf_hat + 1) - torch.log(cdf+1)) ** 2 + self.eps)\n",
        "\n",
        "\n",
        "class WeibullLossMSE(nn.Module):\n",
        "    \"\"\"\n",
        "    y_hat       : predicted RUL\n",
        "    y           : true RUL\n",
        "    y_days      : true age (in days)\n",
        "    lambda_mod  : lambda modifier\n",
        "    eta         : characteristic life\n",
        "    beta        : shape parameter for weibull\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeibullLossMSE, self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, y_hat, y, lambda_mod=2.0, eta=90.0, beta=2.0):\n",
        "\n",
        "        y_hat_days = y - y_hat\n",
        "\n",
        "        # remove any \"inf\" values from when divided by zero\n",
        "        y_hat_days = y_hat_days[torch.isfinite(y_hat_days)]\n",
        "\n",
        "        def weibull_cdf(t, eta, beta):\n",
        "            \"weibull CDF function\"\n",
        "            return 1.0 - torch.exp(-1.0 * ((t / eta) ** beta))\n",
        "\n",
        "        cdf = weibull_cdf(y, eta, beta)\n",
        "        cdf_hat = weibull_cdf(y_hat_days, eta, beta)\n",
        "\n",
        "        return lambda_mod * torch.mean((cdf_hat - cdf) ** 2)\n",
        "\n",
        "\n",
        "class RMSELoss(nn.Module):\n",
        "    # https://discuss.pytorch.org/t/rmse-loss-function/16540/4\n",
        "\n",
        "    def __init__(self, eps=1e-8):\n",
        "        super(RMSELoss, self).__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_hat, y):\n",
        "        return torch.sqrt(self.mse(y_hat, y) + self.eps)\n",
        "\n",
        "\n",
        "class RMSLELoss(nn.Module):\n",
        "    # https://discuss.pytorch.org/t/rmse-loss-function/16540/4\n",
        "\n",
        "    def __init__(self, eps=1e-8):\n",
        "        super(RMSLELoss, self).__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_hat, y):\n",
        "        return torch.sqrt(self.mse(torch.log(y_hat + 1), torch.log(y + 1)) + self.eps)\n",
        "\n",
        "\n",
        "class MAPELoss(nn.Module):\n",
        "\n",
        "    def __init__(self, eps=1e-8):\n",
        "        super(MAPELoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_hat, y):\n",
        "\n",
        "        return torch.mean(torch.abs(y - y_hat) / torch.abs(y + self.eps)) * 100"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Transformer_scratch.ipynb",
      "provenance": [],
      "mount_file_id": "1iXqE5DgNAGs2qMji9MOgFqlUJctTGFO9",
      "authorship_tag": "ABX9TyPpr9UgmCQ855UBkCEZGx4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}